{"cells":[{"cell_type":"markdown","source":["# Massive AAPL Daily Bars Processor\n","- Loads raw table from Copy activity\n","- Adds Date from timestamp\n","- Renames columns to clean names\n","- Dedups on Date\n","- Overwrites table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"02615041-70e0-4884-a9fb-d8b72070c51a"},{"cell_type":"code","source":["from pyspark.sql.functions import col, to_date\n","\n","# Load the existing final table (if it exists)\n","try:\n","    existing_df = spark.read.table(\"aapl_daily_massive\")\n","except:\n","    existing_df = spark.createDataFrame([], schema=\"ticker STRING, volume LONG, vwap DOUBLE, open_price DOUBLE, close_price DOUBLE, high_price DOUBLE, low_price DOUBLE, transactions LONG, Date DATE\")  # empty if table doesn't exist yet\n","\n","# Load the new batch from Notebook 1 (flattened/cleaned)\n","new_df = spark.read.table(\"DailyStockData_Clean\")\n","\n","# Add Date if missing (from timestamp)\n","new_df = new_df.withColumn(\"Date\", to_date(col(\"timestamp\"))) \\\n","               .drop(\"timestamp\")\n","\n","# Rename columns in new batch (to match existing table)\n","new_df = new_df.withColumnRenamed(\"v\", \"volume\") \\\n","               .withColumnRenamed(\"vw\", \"vwap\") \\\n","               .withColumnRenamed(\"o\", \"open_price\") \\\n","               .withColumnRenamed(\"c\", \"close_price\") \\\n","               .withColumnRenamed(\"h\", \"high_price\") \\\n","               .withColumnRenamed(\"l\", \"low_price\") \\\n","               .withColumnRenamed(\"n\", \"transactions\")\n","\n","# Combine existing + new data\n","combined_df = existing_df.unionByName(new_df, allowMissingColumns=True)\n","\n","# Dedup on Date (keep latest version if duplicates)\n","combined_df = combined_df.dropDuplicates([\"Date\"])\n","\n","# Show preview of latest data\n","combined_df.orderBy(\"Date\", ascending=False).show(10, truncate=False)\n","\n","# Overwrite the final table with combined, deduped data\n","combined_df.write.mode(\"overwrite\") \\\n","  .option(\"overwriteSchema\", \"true\") \\\n","  .saveAsTable(\"aapl_daily_massive\")\n","\n","print(\"Daily cleanup complete: new data appended, deduped on Date, table updated. Row count:\", combined_df.count())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_error","livy_statement_state":null,"session_id":null,"normalized_state":"session_error","queued_time":"2026-01-23T20:10:38.5540462Z","session_start_time":"2026-01-23T20:10:38.5550974Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"cce87bc0-c911-49a4-b8ee-2fc6d7f007a8"},"text/plain":"StatementMeta(, , -1, SessionError, , SessionError)"},"metadata":{}},{"output_type":"error","ename":"InvalidHttpRequestToLivy","evalue":"[TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430.","traceback":["InvalidHttpRequestToLivy: [TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430."]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5be1443-98b1-4b34-adc5-dc4c8f321a44"},{"cell_type":"markdown","source":["Last tested: 2026-01-21\n","Columns: Ticker, Open, High, Low, Close, Volume, Date"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b75ee264-506e-4405-8b4b-d0133ebf3da7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"1dbe7af8-a29f-4ef6-a8fb-8147e8b5399d"}],"default_lakehouse":"1dbe7af8-a29f-4ef6-a8fb-8147e8b5399d","default_lakehouse_name":"DailyStockLakehouse","default_lakehouse_workspace_id":"3980c2c7-b4a8-4687-8202-79927f97eaad"}}},"nbformat":4,"nbformat_minor":5}