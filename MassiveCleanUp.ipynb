{"cells":[{"cell_type":"markdown","source":["# Massive AAPL Daily Bars Processor\n","- Loads raw table from Copy activity\n","- Adds Date from timestamp\n","- Renames columns to clean names\n","- Dedups on Date\n","- Overwrites table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"02615041-70e0-4884-a9fb-d8b72070c51a"},{"cell_type":"code","source":["from pyspark.sql.functions import col, to_date\n","\n","# Load the existing final table (if it exists)\n","try:\n","    existing_df = spark.read.table(\"aapl_daily_massive\")\n","except:\n","    existing_df = spark.createDataFrame([], schema=\"ticker STRING, volume LONG, vwap DOUBLE, open_price DOUBLE, close_price DOUBLE, high_price DOUBLE, low_price DOUBLE, transactions LONG, Date DATE\")  # empty if table doesn't exist yet\n","\n","# Load the new batch from Notebook 1 (flattened/cleaned)\n","new_df = spark.read.table(\"DailyStockData_Clean\")\n","\n","# Add Date if missing (from timestamp)\n","new_df = new_df.withColumn(\"Date\", to_date(col(\"timestamp\"))) \\\n","               .drop(\"timestamp\")\n","\n","# Rename columns in new batch (to match existing table)\n","new_df = new_df.withColumnRenamed(\"v\", \"volume\") \\\n","               .withColumnRenamed(\"vw\", \"vwap\") \\\n","               .withColumnRenamed(\"o\", \"open_price\") \\\n","               .withColumnRenamed(\"c\", \"close_price\") \\\n","               .withColumnRenamed(\"h\", \"high_price\") \\\n","               .withColumnRenamed(\"l\", \"low_price\") \\\n","               .withColumnRenamed(\"n\", \"transactions\")\n","\n","# Combine existing + new data\n","combined_df = existing_df.unionByName(new_df, allowMissingColumns=True)\n","\n","# Dedup on Date (keep latest version if duplicates)\n","combined_df = combined_df.dropDuplicates([\"Date\"])\n","\n","# Show preview of latest data\n","combined_df.orderBy(\"Date\", ascending=False).show(10, truncate=False)\n","\n","# Overwrite the final table with combined, deduped data\n","combined_df.write.mode(\"overwrite\") \\\n","  .option(\"overwriteSchema\", \"true\") \\\n","  .saveAsTable(\"aapl_daily_massive\")\n","\n","print(\"Daily cleanup complete: new data appended, deduped on Date, table updated. Row count:\", combined_df.count())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_error","livy_statement_state":null,"session_id":null,"normalized_state":"session_error","queued_time":"2026-01-23T20:10:38.5540462Z","session_start_time":"2026-01-23T20:10:38.5550974Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"cce87bc0-c911-49a4-b8ee-2fc6d7f007a8"},"text/plain":"StatementMeta(, , -1, SessionError, , SessionError)"},"metadata":{}},{"output_type":"error","ename":"InvalidHttpRequestToLivy","evalue":"[TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430.","traceback":["InvalidHttpRequestToLivy: [TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430."]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5be1443-98b1-4b34-adc5-dc4c8f321a44"},{"cell_type":"code","source":["import notebookutils\n","import requests\n","\n","# Hard-coded from your screenshots\n","dataset_name = \"MassiveReport1\"\n","workspace_name = \"MassiveProject\"\n","\n","def final_refresh_solution():\n","    try:\n","        # Use notebookutils (the universal Fabric tool) to get your token\n","        token = notebookutils.credentials.getToken(\"pbi\")\n","        headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n","        \n","        # Get the workspace ID from the notebook context\n","        ws_id = notebookutils.runtime.context[\"currentWorkspaceId\"]\n","        \n","        # Trigger the refresh using the direct name URL\n","        # This is the most stable 'God Mode' path in Fabric\n","        refresh_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{ws_id}/datasets/{dataset_name}/refreshes\"\n","        \n","        response = requests.post(refresh_url, headers=headers)\n","        \n","        if response.status_code == 202:\n","            print(f\"✅ SUCCESS! {dataset_name} is now refreshing.\")\n","            print(\"Automation is fixed. You can safely delete the old pipeline activities.\")\n","        else:\n","            # If the name doesn't work, it might need the ID, but name usually works in Fabric\n","            print(f\"❌ Status {response.status_code}. Checking for ID instead...\")\n","            ds_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{ws_id}/datasets\"\n","            ds_resp = requests.get(ds_url, headers=headers)\n","            datasets = ds_resp.json().get(\"value\", [])\n","            target_id = next((d[\"id\"] for d in datasets if d[\"name\"] == dataset_name), None)\n","            \n","            if target_id:\n","                final_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{ws_id}/datasets/{target_id}/refreshes\"\n","                requests.post(final_url, headers=headers)\n","                print(f\"✅ SUCCESS! Refreshed via ID: {target_id}\")\n","            else:\n","                print(\"❌ Could not find the model. Verify the name 'MassiveReport1'.\")\n","\n","    except Exception as e:\n","        print(f\"❌ Error: {e}\")\n","\n","final_refresh_solution()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"71f24e21-a1cc-4dbe-8767-d4c84e5f1dfd","normalized_state":"finished","queued_time":"2026-02-14T14:17:52.1042266Z","session_start_time":null,"execution_start_time":"2026-02-14T14:17:52.105497Z","execution_finish_time":"2026-02-14T14:17:53.4311236Z","parent_msg_id":"4e913004-837a-4f89-9886-91578db5cd2e"},"text/plain":"StatementMeta(, 71f24e21-a1cc-4dbe-8767-d4c84e5f1dfd, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["❌ Status 404. Checking for ID instead...\n✅ SUCCESS! Refreshed via ID: e0d11723-2a5a-4305-b27c-35603cc134ca\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71714c59-281f-4732-9f20-e15a469cc275"},{"cell_type":"markdown","source":["Last tested: 2026-01-21\n","Columns: Ticker, Open, High, Low, Close, Volume, Date"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b75ee264-506e-4405-8b4b-d0133ebf3da7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"1dbe7af8-a29f-4ef6-a8fb-8147e8b5399d"}],"default_lakehouse":"1dbe7af8-a29f-4ef6-a8fb-8147e8b5399d","default_lakehouse_name":"DailyStockLakehouse","default_lakehouse_workspace_id":"3980c2c7-b4a8-4687-8202-79927f97eaad"}}},"nbformat":4,"nbformat_minor":5}